# BronchoTransformer: Sequential Vision Transformer for 7-DoF Pose Estimation

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/license-MIT-green)

Este repositorio contiene la implementaci√≥n oficial de **BronchoTransformer**, una arquitectura basada en **Vision Transformers (ViT) Secuenciales** dise√±ada para la estimaci√≥n de pose en broncoscopia virtual.

A diferencia de los m√©todos tradicionales que utilizan redes recurrentes (RNNs) o restricciones geom√©tricas expl√≠citas, este modelo aprende dependencias temporales globales directamente de secuencias de video, logrando una **precisi√≥n de localizaci√≥n superior (3.12 mm)** y una mayor estabilidad sin necesidad de sensores externos.

---

## üìã Tabla de Contenidos
- [Introducci√≥n](#introducci√≥n)
- [Preparaci√≥n del Dataset](#preparaci√≥n-del-dataset)
- [Instalaci√≥n](#instalaci√≥n)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Uso](#uso)
  - [Entrenamiento](#entrenamiento)
  - [Evaluaci√≥n](#evaluaci√≥n)

---

## Introducci√≥n

El objetivo de este proyecto es estimar la pose de la c√°mara (Posici√≥n $x,y,z$ y Orientaci√≥n $q_w, q_x, q_y, q_z$) dentro del √°rbol bronquial utilizando √∫nicamente informaci√≥n visual.

El modelo utiliza un enfoque **End-to-End**:
1.  **Extractor Espacial:** Un ViT procesa cada *frame* individualmente.
2.  **Codificador Temporal:** Un Transformer Encoder modela la secuencia de movimiento.
3.  **Cabezal de Regresi√≥n:** Predice el vector de pose de 7 grados de libertad (7-DoF).

Para probar el modelo, es necesario descargar el dataset sint√©tico del paper **BronchoPose** de Borrego et al. Puedes obtenerlo aqu√≠:
[Descargar Dataset BronchoPose](https://dataverse.csuc.cat/dataset.xhtml?persistentId=doi:10.34810/data2251)

---

## Preparaci√≥n del Dataset

El modelo espera una estructura de directorios espec√≠fica basada en el dataset *VirtualNavigations*.

1.  **Descarga:** Baja los archivos del dataset desde el enlace superior. Nota que llegan en *split zips* (partes divididas).
2.  **Descompresi√≥n:** Une los archivos zip para extraer el contenido completo.
3.  **Organizaci√≥n:**
    * Crea una carpeta llamada `data` en la ra√≠z de este repositorio.
    * Mueve la carpeta descomprimida `VirtualNavigations` dentro de `data`.
    
La estructura final debe verse as√≠:
```text
BronchoTransformer/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ VirtualNavigations/
‚îÇ       ‚îú‚îÄ‚îÄ LENS_P1_14_01_2016_INSP_CPAP/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Frames/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ P1_r1_to_3.csv
‚îÇ       ‚îú‚îÄ‚îÄ ...
```
## Instalaci√≥n
1. Clonar el repositorio:
git clone [https://github.com/tu-usuario/BronchoTransformer.git](https://github.com/tu-usuario/BronchoTransformer.git)
cd BronchoTransformer
2. Instalar dependencias: Se recomienda usar un entorno virtual (conda o venv). Ejecuta el siguiente comando para instalar las librer√≠as necesarias:
pip install torch torchvision timm numpy pandas matplotlib opencv-python tqdm

## Estructura del proyecto
model.py: Define la arquitectura SequentialPoseTransformer.
dataset.py: Clase BronchoscopyDataset para cargar secuencias de im√°genes y etiquetas de pose.
train.py: Script principal para entrenar el modelo. Incluye Early Stopping y guardado de checkpoints.
evaluate_metrics.py: Calcula el Error de Posici√≥n (mm) y el Error de Direcci√≥n (Grados) en el set de prueba.
plotlosses.py: Utilidad para graficar las curvas de p√©rdida de entrenamiento y validaci√≥n.

## Uso

Entrenamiento:
Para entrenar el modelo desde cero, ejecuta el script train.py. Puedes ajustar los hiperpar√°metros (batch size, epochs, learning rate) editando la secci√≥n de configuraci√≥n dentro del archivo.

Esto generar√°:
bronco_model.pth: El archivo con los pesos del modelo entrenado.
loss_curve.png: Gr√°fica de la evoluci√≥n del entrenamiento.

## Evaluaci√≥n
Para evaluar el desempe√±o del modelo con las m√©tricas del estado del arte, utiliza evaluate_metrics.py.

Nota sobre la M√©trica de Direcci√≥n: Este script asume que el eje √≥ptico de la c√°mara virtual corresponde al Eje X local (+1, 0, 0), basado en un an√°lisis geom√©trico del dataset BronchoPose. El error de direcci√≥n se calcula ignorando la rotaci√≥n sobre el propio eje (roll), priorizando el vector de vista hacia la trayectoria bronquial.
